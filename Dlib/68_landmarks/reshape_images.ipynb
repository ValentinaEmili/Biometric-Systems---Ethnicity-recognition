{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJuJd6nadztS"
      },
      "outputs": [],
      "source": [
        "%pip install gdown\n",
        "import gdown\n",
        "\n",
        "url = \"https://drive.google.com/drive/folders/1I34vsWKtKYZTyxsaHSx3n-zzAmJaOpf4?usp=share_link\"\n",
        "gdown.download_folder(url, quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "!pip install dlib"
      ],
      "metadata": {
        "id": "rK0ceiSufImR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "def weighted_average_dimensions(folder):\n",
        "  total_weight, weighted_sum_h, weighted_sum_w = 0, 0, 0\n",
        "  for file_name in os.listdir(folder):\n",
        "      file_path = os.path.join(folder, file_name)\n",
        "      image = cv2.imread(file_path)\n",
        "      height, width = image.shape[:2]\n",
        "      weight = height * width\n",
        "      total_weight += weight\n",
        "      weighted_sum_h += height * weight\n",
        "      weighted_sum_w += width * weight\n",
        "  average_height = weighted_sum_h / total_weight\n",
        "  average_width = weighted_sum_w / total_weight\n",
        "\n",
        "  return int(average_height), int(average_width)\n",
        "\n",
        "def resize_images(feature, folder, target_size):\n",
        "  output_folder = f\"resized_{feature}\"\n",
        "  os.makedirs(output_folder, exist_ok=True)\n",
        "  for file_name in os.listdir(folder):\n",
        "    file_path = os.path.join(folder, file_name)\n",
        "    image = cv2.imread(file_path)\n",
        "    resized_image = cv2.resize(image, target_size)\n",
        "    output_path = os.path.join(output_folder, file_name)\n",
        "    cv2.imwrite(output_path, resized_image)\n",
        "\n",
        "def main():\n",
        "  folder_path = \"/content/68_landmarks\"\n",
        "  for file_name in os.listdir(folder_path):\n",
        "\n",
        "    # zipped files in the folder biometric_systems_dataset\n",
        "    if file_name.endswith(\".zip\"):\n",
        "\n",
        "      # full path to the zip file\n",
        "      file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "      # create folder in the same path with the name of the zip file\n",
        "      extract_folder = os.path.join(folder_path, file_name.replace(\".zip\", \"\"))\n",
        "      os.makedirs(extract_folder, exist_ok=True)\n",
        "\n",
        "      # open and extract the file\n",
        "      with zipfile.ZipFile(file_path, mode=\"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_folder)\n",
        "\n",
        "  features = [\"eyes\", \"mouth\", \"nose\"]\n",
        "  for feature in features:\n",
        "    feature_folder_path = os.path.join(folder_path, feature)\n",
        "    average_height, average_width = weighted_average_dimensions(feature_folder_path)\n",
        "    resize_images(feature, feature_folder_path, (average_width, average_height))\n",
        "    shutil.make_archive(f\"resized_{feature}\", \"zip\",f\"resized_{feature}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "uEN3fTGTfONq"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}