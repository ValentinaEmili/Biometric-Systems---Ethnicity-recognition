{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import warnings\n",
    "from PIL import Image\n",
    "import joblib\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_predictor_path = \"//Users/valentinaemili/Desktop/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector() # HOG-based detector\n",
    "predictor = dlib.shape_predictor(shape_predictor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_normal(shape):\n",
    "  shape_normal = []\n",
    "  for i in range(0, shape.num_parts):\n",
    "    shape_normal.append((i, (shape.part(i).x, shape.part(i).y)))\n",
    "  return shape_normal\n",
    "\n",
    "def get_eyes_nose_dlib(shape):\n",
    "  nose = shape[30][1]\n",
    "  left_eye_x = (shape[36][1][0] + shape[39][1][0]) // 2\n",
    "  left_eye_y = (shape[36][1][1] + shape[39][1][1]) // 2\n",
    "  right_eyes_x = (shape[42][1][0] + shape[45][1][0]) // 2\n",
    "  right_eyes_y = (shape[42][1][1] + shape[45][1][1]) // 2\n",
    "\n",
    "  return nose, (left_eye_x, left_eye_y), (right_eyes_x, right_eyes_y)\n",
    "\n",
    "def rotate_points(shape, rotation_matrix):\n",
    "  points = []\n",
    "  for i in range(0, 68):\n",
    "    x,y = shape[i][1]\n",
    "    points.append((x, y, 1))\n",
    "  points = np.array(points)\n",
    "  rotated_points = np.dot(rotation_matrix, points.T).T\n",
    "  return np.array([(int(p[0]), int(p[1])) for p in rotated_points])\n",
    "\n",
    "def get_eyes_in_image(image, points):\n",
    "  height, width = image.shape[:2]\n",
    "  top = points[17:27][np.argmin(points[17:27, 1])]\n",
    "  bottom = points[29]\n",
    "  min_x, min_y = np.min(points[36:48, 0]), np.min(points[36:48, 1])\n",
    "  max_x, max_y = np.max(points[36:48, 0]), np.max(points[36:48, 1])\n",
    "  min_x, min_y = max(0, min_x - 10), max(top[1], min_y - 10)\n",
    "  max_x, max_y = min(width, max_x + 10), min(bottom[1], max_y + 10)\n",
    "  image = image[min_y:max_y, min_x:max_x]\n",
    "  return image\n",
    "\n",
    "def get_nose_in_image(image, points):\n",
    "  height, width = image.shape[:2]\n",
    "  top = points[36:48][np.argmin(points[36:48, 1])]\n",
    "  bottom = points[48:68][np.argmax(points[48:68, 1])]\n",
    "  min_x, min_y = np.min(points[27:36, 0]), np.min(points[27:36, 1])\n",
    "  max_x, max_y = np.max(points[27:36, 0]), np.max(points[27:36, 1])\n",
    "  min_x, min_y = max(0, min_x - 5), max(top[1], min_y - 5)\n",
    "  max_x, max_y = min(width, max_x + 5), min(bottom[1], max_y + 5)\n",
    "  image = image[min_y:max_y, min_x:max_x]\n",
    "  return image\n",
    "\n",
    "def get_mouth_in_image(image, points):\n",
    "  height, width = image.shape[:2]\n",
    "  top = points[27:36][np.argmin(points[27:36, 1])]\n",
    "  min_x, min_y = np.min(points[48:68, 0]), np.min(points[48:68, 1])\n",
    "  max_x, max_y = np.max(points[48:68, 0]), np.max(points[48:68, 1])\n",
    "  min_x, min_y = max(0, min_x - 10), max(top[1], min_y - 10)\n",
    "  max_x, max_y = min(width, max_x + 10), min(height, max_y + 10)\n",
    "  image = image[min_y:max_y, min_x:max_x]\n",
    "  return image\n",
    "\n",
    "def align_face(image):\n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  faces = detector(gray)\n",
    "  if len(faces) > 0:\n",
    "    for face in faces:\n",
    "      shape = predictor(gray, face)\n",
    "      shape = shape_to_normal(shape)\n",
    "      nose, left_eye, right_eye = get_eyes_nose_dlib(shape)\n",
    "      delta_x = right_eye[0] - left_eye[0]\n",
    "      delta_y = right_eye[1] - left_eye[1]\n",
    "      angle = math.atan2(delta_y, delta_x) * (180 / math.pi)\n",
    "      center_of_eyes = ((left_eye[0] + right_eye[0]) // 2, (left_eye[1] + right_eye[1]) // 2)\n",
    "      rotation_matrix = cv2.getRotationMatrix2D(center_of_eyes, angle, scale=1.0)\n",
    "      h, w = image.shape[:2]\n",
    "      aligned_image = cv2.warpAffine(image, rotation_matrix, (w, h), flags=cv2.INTER_LINEAR)\n",
    "      rotated_points = rotate_points(shape, rotation_matrix)\n",
    "      eyes_image = get_eyes_in_image(aligned_image, rotated_points)\n",
    "      nose_image = get_nose_in_image(aligned_image, rotated_points)\n",
    "      mouth_image = get_mouth_in_image(aligned_image, rotated_points)\n",
    "      if eyes_image.size == 0:\n",
    "        continue\n",
    "      if nose_image.size == 0:\n",
    "        continue\n",
    "      if mouth_image.size == 0:\n",
    "        continue\n",
    "\n",
    "      # boolean=True, image is aligned\n",
    "      return True, eyes_image, nose_image, mouth_image\n",
    "  return False, _, _, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*InconsistentVersionWarning.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*Secure coding is automatically enabled.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model pre-trained on ImageNet dataset\n",
    "resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
    "# modify the fully connected layer\n",
    "resnet50.fc = torch.nn.Identity()\n",
    "resnet50.eval().to(device)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_features(image):\n",
    "  image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "  with torch.no_grad():\n",
    "    features = resnet50(image_tensor)\n",
    "  return features.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethnicity, age and gender recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  folder_path = \"/Users/valentinaemili/Desktop/biometric_systems_project/trained_model\"\n",
    "  category = [\"age\", \"ethnicity\", \"gender\"]\n",
    "  features = [\"eyes\", \"mouth\", \"nose\"]\n",
    "  ages = [\"around 3 yrs\", \"around 9 yrs\", \"around 17 yrs\", \"around 28 yrs\", \"around 43 yrs\",  \"around 63 yrs\", \"around 83 yrs\", \"around 104 yrs\"]\n",
    "  ethnicities = [\"White\", \"Black\", \"Asian\", \"Indian\", \"Hispanic/Latino/Middle Eastern\"]\n",
    "  gender = [\"male\", \"female\"]\n",
    "  models = {}\n",
    "  # Load models for each characteristic and feature combination\n",
    "  for c in category:\n",
    "    for feature in features:\n",
    "      feature_path = os.path.join(folder_path, c, f\"svm_model_{feature}.pkl\")\n",
    "      model = joblib.load(feature_path)\n",
    "      models[(c, feature)] = model\n",
    "\n",
    "  cv2.setUseOptimized(True)\n",
    "  cv2.namedWindow('Video Frame with Predictions', cv2.WND_PROP_FULLSCREEN)\n",
    "  timeout_duration = 10  # Close the window after 10 seconds\n",
    "  cap = cv2.VideoCapture(0)\n",
    "  while True:\n",
    "    ret, img = cap.read() # ret is bool\n",
    "    if ret:\n",
    "      boolean, eyes_image, nose_image, mouth_image = align_face(img)\n",
    "      if boolean:\n",
    "        eyes_image = Image.fromarray(eyes_image)\n",
    "        nose_image = Image.fromarray(nose_image)\n",
    "        mouth_image = Image.fromarray(mouth_image)\n",
    "\n",
    "        eyes_features = extract_and_save_features(eyes_image)\n",
    "        nose_features = extract_and_save_features(nose_image)\n",
    "        mouth_features = extract_and_save_features(mouth_image)\n",
    "        predictions_text = \"\"\n",
    "\n",
    "        for c in category:\n",
    "          for feature in features:\n",
    "            model = models[(c, feature)]\n",
    "            # Select the appropriate feature data\n",
    "            if feature == \"eyes\":\n",
    "              feature_data = eyes_features\n",
    "            elif feature == \"nose\":\n",
    "              feature_data = nose_features\n",
    "            elif feature == \"mouth\":\n",
    "              feature_data = mouth_features\n",
    "            # Perform prediction\n",
    "            prediction = model.predict([feature_data])\n",
    "            \n",
    "            if c == \"age\":\n",
    "              age_pred = ages[int(prediction)]\n",
    "              predictions_text += f\"{feature} ({c}): {age_pred}\\n\"\n",
    "            elif c == \"ethnicity\":\n",
    "              ethn_pred = ethnicities[int(prediction)]\n",
    "              predictions_text += f\"{feature} ({c}): {ethn_pred}\\n\"\n",
    "            elif c == \"gender\":\n",
    "              gender_pred = gender[int(prediction)]\n",
    "              predictions_text += f\"{feature} ({c}): {gender_pred}\\n\"\n",
    "        y_offset = 30\n",
    "        for line in predictions_text.splitlines():\n",
    "          cv2.putText(img, line, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "          y_offset += 30\n",
    "        \n",
    "        cv2.imshow('Video Frame with Predictions', img)\n",
    "        k = cv2.waitKey(10)\n",
    "        if k == ord('q'):\n",
    "          break\n",
    "  cap.release()\n",
    "  cv2.destroyAllWindows()\n",
    "  cv2.waitKey(1)\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
